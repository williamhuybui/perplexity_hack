{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b57e15d",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d76f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Local\\Temp\\ipykernel_24068\\3124296229.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceEmbeddings(\n",
      "C:\\Users\\alice\\AppData\\Local\\Temp\\ipykernel_24068\\3124296229.py:35: LangChainDeprecationWarning: The class `ChatPerplexity` was deprecated in LangChain 0.3.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-perplexity package and should be used instead. To use it run `pip install -U :class:`~langchain-perplexity` and import as `from :class:`~langchain_perplexity import ChatPerplexity``.\n",
      "  llm = ChatPerplexity(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Based on the provided information, Huy Bui's top skills appear to be in **data science and programming**, particularly in:\n",
      "\n",
      "- **Python programming**: Utilizing libraries like Pandas, Numpy, and Scikit-learn.\n",
      "- **Data analysis and modeling**: Developing regression models and leveraging machine learning techniques.\n",
      "- **Cloud computing**: Proficient in AWS services such as S3 and Lambda.\n",
      "- **Database management**: Skilled in SQL and BigQuery.\n",
      "- **Web development**: Experienced with React and JavaScript for UI/UX enhancements.\n",
      "- **Leadership and team management**: Proven ability to lead cross-functional teams in fast-paced environments.\n",
      "\n",
      "However, if you are referring to a different Huy Bui, such as the professor at Parsons or the barber, their skills would be different and not detailed in the provided context.\n",
      "############################\n",
      "######DOC 1######\n",
      "page_content='Huy Bui\n",
      "williamhuybui@gmail.com | linkedin.com/in/huy-bui-ds\n",
      "Experience\n",
      "Publicis Groupe\n",
      "Remote\n",
      "Senior Data Scientist\n",
      "June 2022 – Present\n",
      "– Developed 7 client-agnostic applications using Python and Dash, empowering 100+ analysts to streamline data\n",
      "analysis and reporting, saving over 4,600 hours of productivity.\n",
      "– Led data science initiatives and cross-functional teams to build scalable analytics solutions, driving alignment and\n",
      "efficiency in a fast-paced, startup-like environment.\n",
      "– Built an automated data pipeline leveraging Python, SQL, BigQuery, AWS Lambda, S3, and EventBridge.\n",
      "– Developed regression models and LLM-based procedures to extract insights and optimize marketing spend\n",
      "allocation for global enterprises.\n",
      "– Enhanced UI/UX by migrating Python codebases to React, using tools and techniques such as Figma, Postman,\n",
      "and REST APIs.\n",
      "Enovate Upstream\n",
      "Remote\n",
      "Research Data Scientist\n",
      "April 2020 – June 2022\n",
      "– Lead author of the research Machine Learning Applications to Improve Pore Pressure Prediction in Hazardous\n",
      "Drilling Environments, published at OnePetro.\n",
      "– Developed time series and physics hybrid-models for drilling measurement predictions, reducing reliance on costly\n",
      "logging tools and saving clients over $200,000.\n",
      "– Built a robust data cleaning and standardization pipeline in Python, streamlining data preparation across multiple\n",
      "clients and reducing manual processing time by 30%.\n",
      "Texas A&M University\n",
      "College Station, Texas\n",
      "Teaching Assistant\n",
      "August 2017 – May 2019\n",
      "– Taught Python, Matlab, Calculus I, II, Differential Equations, and Discrete Mathematics to engineering students,\n",
      "assisting with lectures and grading assessments.\n",
      "Technical Skills\n",
      "Data Science and Programming: Python (Pandas, Numpy, Scikit-learn), SQL, JavaScript, React, BigQuery, LLM.\n",
      "Other: OpenAI api, AWS (S3, Redshift), CI/CD (TeamCity, GitHub).\n",
      "Education\n",
      "Texas A&M University\n",
      "College Station, Texas\n",
      "Master of Science in Mathematics\n",
      "July 2019\n",
      "– Final thesis: Discharging Method on the Planar Graph.\n",
      "University of Houston - Downtown\n",
      "Houston, Texas\n",
      "Bachelor of Science in Mathematics\n",
      "May 2017\n",
      "– Summa Cum Laude, minor in Computer Science.\n",
      "– 4 undergraduate researches in Quantum Calculus.\n",
      "– President of Math club (3 years), Scholars Academy member, Pi Mu Epsilon member.\n",
      "Recognition and Activities\n",
      "1st Place Team - Catalyst Award in Centralized Data | Publicis Groupe - CBS Studio\n",
      "December 2024\n",
      "– Recognized for developing a scalable data analytics suite that enhances decision-making and bridges siloed data.\n",
      "1st Place Team - Publicis Global Data Intelligence Awards | Publicis Groupe\n",
      "November 2024\n",
      "– Recognized the most innovative analytics solution across Publicis Groupe.\n",
      "Mentor | Techsphere\n",
      "August 2022 - August 2024\n",
      "– Helped Vietnamese professionals with mock interviews in data science and software engineering.\n",
      "1st Place Team at Glasstire DataHack | Rice University\n",
      "December 2019\n",
      "3rd Place Team at TAMU Hack | Texas A&M University\n",
      "January 2020' metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-19T01:02:47+00:00', 'source': 'data\\\\Huy_Bui_Resume.pdf', 'file_path': 'data\\\\Huy_Bui_Resume.pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-05-19T01:02:47+00:00', 'trapped': '', 'modDate': 'D:20250519010247Z', 'creationDate': 'D:20250519010247Z', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatPerplexity\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "\n",
    "#Loader\n",
    "loader = PyMuPDFLoader(\"data\\Huy_Bui_Resume.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "#Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "#Embbedding\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False} #False Euclidean, True cosine similarity\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "#Vector Store\n",
    "vector_store = FAISS.from_documents(chunks, hf)\n",
    "vector_store.save_local(\"faiss_index_open\")\n",
    "\n",
    "#Retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "#LLM\n",
    "llm = ChatPerplexity(\n",
    "    model=\"sonar\",\n",
    "    pplx_api_key = \"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\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "query = \"How many times do the vowels 'ae' appear in 'I am a data analyst and engineer'?\"\n",
    "query = \"What are Huy Bui’s certifications from the resume?\"\n",
    "query = \"Dont make up the answer if you dont know. Question: What are Huy Bui's top skills?\"\n",
    "response = qa_chain.invoke(query)\n",
    "\n",
    "print(\"Response\", response['result'])\n",
    "print(\"############################\")\n",
    "for i, doc in enumerate(response['source_documents']):\n",
    "    print(f\"######DOC {i+1}######\")\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798d4e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script websockets.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script wsdump.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script uvicorn.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script humanfriendly.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pyproject-build.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script watchfiles.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script coloredlogs.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script typer.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script onnxruntime_test.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jsonschema.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script fastapi.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts opentelemetry-bootstrap.exe and opentelemetry-instrument.exe are installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script chroma.exe is installed in 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Lan Dao\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.10-cp39-abi3-win_amd64.whl (19.0 MB)\n",
      "     --------------------------------------- 19.0/19.0 MB 16.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb) (2.0.2)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.5/62.5 KB ? eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl (12 kB)\n",
      "Collecting overrides>=7.3.1\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Collecting build>=1.0.3\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.8/65.8 KB ? eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl (18 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "     ---------------------------------------- 243.2/243.2 KB ? eta 0:00:00\n",
      "Collecting fastapi==0.115.9\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "     ---------------------------------------- 94.9/94.9 KB 5.6 MB/s eta 0:00:00\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 15.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "     ---------------------------------------- 152.8/152.8 KB ? eta 0:00:00\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb) (2.11.4)\n",
      "Collecting typer>=0.9.0\n",
      "  Downloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.3/45.3 KB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Downloading mmh3-5.1.0-cp39-cp39-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.5/41.5 KB ? eta 0:00:00\n",
      "Collecting grpcio>=1.58.0\n",
      "  Downloading grpcio-1.71.0-cp39-cp39-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 17.1 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
      "     ---------------------------------------- 119.0/119.0 KB ? eta 0:00:00\n",
      "Collecting jsonschema>=4.19.0\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "     ---------------------------------------- 88.5/88.5 KB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\lan dao\\appdata\\roaming\\python\\python39\\site-packages (from chromadb) (4.13.2)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Collecting pypika>=0.48.9\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 67.3/67.3 KB 3.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting posthog>=2.4.0\n",
      "  Downloading posthog-4.2.0-py2.py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.7/96.7 KB ? eta 0:00:00\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.19.2-cp39-cp39-win_amd64.whl (11.1 MB)\n",
      "     --------------------------------------- 11.1/11.1 MB 15.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Collecting starlette<0.46.0,>=0.40.0\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.5/71.5 KB ? eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\lan dao\\appdata\\roaming\\python\\python39\\site-packages (from build>=1.0.3->chromadb) (8.7.0)\n",
      "Collecting tomli>=1.1.0\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Collecting pyproject_hooks\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lan dao\\appdata\\roaming\\python\\python39\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: anyio in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.25.1-cp39-cp39-win_amd64.whl (231 kB)\n",
      "     ------------------------------------- 231.5/231.5 KB 13.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 KB 8.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\lan dao\\appdata\\roaming\\python\\python39\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Downloading google_auth-2.40.2-py2.py3-none-any.whl (216 kB)\n",
      "     ------------------------------------- 216.1/216.1 KB 12.9 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.8/58.8 KB ? eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\lan dao\\appdata\\roaming\\python\\python39\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Collecting durationpy>=0.7\n",
      "  Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 46.0/46.0 KB ? eta 0:00:00\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.31.0-cp39-cp39-win_amd64.whl (435 kB)\n",
      "     ------------------------------------- 435.2/435.2 KB 28.3 MB/s eta 0:00:00\n",
      "Collecting deprecated>=1.2.6\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting importlib-metadata>=4.6\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\n",
      "Collecting googleapis-common-protos~=1.52\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "     ------------------------------------- 294.5/294.5 KB 17.8 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-proto==1.33.1\n",
      "  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 55.9/55.9 KB 3.0 MB/s eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-5.29.4-cp39-cp39-win_amd64.whl (434 kB)\n",
      "     -------------------------------------- 434.6/434.6 KB 9.2 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation-asgi==0.54b1\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl (16 kB)\n",
      "Collecting opentelemetry-instrumentation==0.54b1\n",
      "  Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl (31 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
      "     ------------------------------------- 194.9/194.9 KB 11.5 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-util-http==0.54b1\n",
      "  Downloading opentelemetry_util_http-0.54b1-py3-none-any.whl (7.3 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Collecting asgiref~=3.0\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Collecting backoff>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.5/87.5 KB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lan dao\\appdata\\roaming\\python\\python39\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.31.4)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting click<8.2,>=8.0.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 98.2/98.2 KB 5.9 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-15.0.1-cp39-cp39-win_amd64.whl (176 kB)\n",
      "     ------------------------------------- 176.8/176.8 KB 10.4 MB/s eta 0:00:00\n",
      "Collecting httptools>=0.6.3\n",
      "  Downloading httptools-0.6.4-cp39-cp39-win_amd64.whl (89 kB)\n",
      "     ---------------------------------------- 89.6/89.6 KB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Collecting watchfiles>=0.13\n",
      "  Downloading watchfiles-1.0.5-cp39-cp39-win_amd64.whl (292 kB)\n",
      "     ------------------------------------- 292.0/292.0 KB 18.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\lan dao\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources->chromadb) (3.21.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.3/181.3 KB 10.7 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\lan dao\\appdata\\roaming\\python\\python39\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.8/86.8 KB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lan dao\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.2/83.2 KB 4.6 MB/s eta 0:00:00\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.1/83.1 KB ? eta 0:00:00\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53914 sha256=f3f21a08d893d7d1c5f979207e61806e63c6e343743e7469e461a304c53fe128\n",
      "  Stored in directory: c:\\users\\lan dao\\appdata\\local\\pip\\cache\\wheels\\f7\\02\\64\\d541eac67ec459309d1fb19e727f58ecf7ffb4a8bf42d4cfe5\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, wrapt, websockets, websocket-client, tomli, shellingham, rpds-py, pyreadline3, pyproject_hooks, pyasn1, protobuf, overrides, opentelemetry-util-http, oauthlib, mmh3, mdurl, importlib-resources, importlib-metadata, httptools, grpcio, click, cachetools, bcrypt, backoff, asgiref, uvicorn, rsa, requests-oauthlib, referencing, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, humanfriendly, googleapis-common-protos, deprecated, build, watchfiles, starlette, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonschema-specifications, google-auth, coloredlogs, typer, opentelemetry-semantic-conventions, onnxruntime, kubernetes, jsonschema, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.7.0\n",
      "    Uninstalling importlib_metadata-8.7.0:\n",
      "      Successfully uninstalled importlib_metadata-8.7.0\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 chromadb-1.0.10 click-8.1.8 coloredlogs-15.0.1 deprecated-1.2.18 durationpy-0.10 fastapi-0.115.9 flatbuffers-25.2.10 google-auth-2.40.2 googleapis-common-protos-1.70.0 grpcio-1.71.0 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.6.1 importlib-resources-6.5.2 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 kubernetes-32.0.1 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.1.0 oauthlib-3.2.2 onnxruntime-1.19.2 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-grpc-1.33.1 opentelemetry-instrumentation-0.54b1 opentelemetry-instrumentation-asgi-0.54b1 opentelemetry-instrumentation-fastapi-0.54b1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 opentelemetry-util-http-0.54b1 overrides-7.7.0 posthog-4.2.0 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 referencing-0.36.2 requests-oauthlib-2.0.0 rich-14.0.0 rpds-py-0.25.1 rsa-4.9.1 shellingham-1.5.4 starlette-0.45.3 tomli-2.2.1 typer-0.15.4 uvicorn-0.34.2 watchfiles-1.0.5 websocket-client-1.8.0 websockets-15.0.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d04483",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf5c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8156c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1182 total documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Local\\Temp\\ipykernel_22968\\2395293585.py:45: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceBgeEmbeddings(\n",
      "C:\\Users\\alice\\AppData\\Local\\Temp\\ipykernel_22968\\2395293585.py:84: LangChainDeprecationWarning: The class `ChatPerplexity` was deprecated in LangChain 0.3.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-perplexity package and should be used instead. To use it run `pip install -U :class:`~langchain-perplexity` and import as `from :class:`~langchain_perplexity import ChatPerplexity``.\n",
      "  perplexity_llm = ChatPerplexity(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatPerplexity\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# === Settings ===\n",
    "PDF_FOLDER = \"data\"\n",
    "CHUNK_SIZE = 5000\n",
    "CHUNK_OVERLAP = 500\n",
    "VECTOR_STORE_DIR = \"chroma_index_finance\"\n",
    "LOG_FILE = \"qa_log.csv\"\n",
    "\n",
    "# === Load all PDFs ===\n",
    "all_documents = []\n",
    "for filename in os.listdir(PDF_FOLDER):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(PDF_FOLDER, filename)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        for doc in documents:\n",
    "            doc.metadata[\"source\"] = filename  # track which PDF it came from\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} total documents.\")\n",
    "\n",
    "# === Split text into chunks ===\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# Add unique chunk IDs and ensure source is in metadata\n",
    "for i, doc in enumerate(chunks):\n",
    "    doc.metadata[\"chunk_id\"] = i\n",
    "    doc.metadata[\"source\"] = doc.metadata.get(\"source\", \"unknown\")\n",
    "\n",
    "# === Embedding model ===\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# === Vector Store ===\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=hf,\n",
    "    persist_directory=VECTOR_STORE_DIR\n",
    ")\n",
    "# vector_store.persist()\n",
    "\n",
    "# === Prompt Template (only answer output) ===\n",
    "prompt_template = \"\"\"\n",
    "You are a professional financial advisor with expertise in corporate finance, investment analysis, and career development in finance-related roles.\n",
    "\n",
    "Use only the information provided in the context to answer the user's question.\n",
    "Do not make assumptions or fabricate any details.\n",
    "\n",
    "Respond clearly and professionally, as if advising a client on their financial career or investment decisions.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the answer is not explicitly stated in the context, respond with: \"I don't know based on the provided document\".\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# === Retriever Setup ===\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Perplexity LLM\n",
    "perplexity_llm = ChatPerplexity(\n",
    "    model=\"sonar\",\n",
    "    pplx_api_key=\"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\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Compression retriever\n",
    "compressor = LLMChainExtractor.from_llm(perplexity_llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# === QA Chain ===\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=perplexity_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# === Process QA + Save to CSV ===\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9f5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def process_answer(query):\n",
    "    response = qa_chain({\"query\": query})\n",
    "    \n",
    "    # Handle case where result is a JSON-formatted string\n",
    "    try:\n",
    "        result = json.loads(response[\"result\"])\n",
    "        answer_text = result.get(\"answer\", response[\"result\"])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Fallback: use the raw string if not JSON\n",
    "        answer_text = response[\"result\"]\n",
    "\n",
    "    source_docs = response['source_documents']\n",
    "\n",
    "    sources_info = []\n",
    "    for doc in source_docs:\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\", \"N/A\")\n",
    "        source = doc.metadata.get(\"source\", \"N/A\")\n",
    "        sources_info.append({\"chunk_id\": str(chunk_id), \"source\": source})\n",
    "\n",
    "    # Prepare data to log\n",
    "    top_k_chunks = [src[\"chunk_id\"] for src in sources_info if src[\"chunk_id\"] != \"N/A\"]\n",
    "    sources_list = [src[\"source\"] for src in sources_info if src[\"source\"] != \"N/A\"]\n",
    "\n",
    "    # Write to CSV\n",
    "    file_exists = os.path.isfile(LOG_FILE)\n",
    "    with open(LOG_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"question\", \"answer\", \"sources\", \"top_k_chunks\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow({\n",
    "            \"question\": query,\n",
    "            \"answer\": answer_text,\n",
    "            \"sources\": \"; \".join(list(set(sources_list))),\n",
    "            \"top_k_chunks\": \"; \".join(list(set(top_k_chunks)))\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f269bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Local\\Temp\\ipykernel_22968\\14775218.py:6: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"query\": query})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "questions= pd.read_csv(\"session_1/questions.csv\")\n",
    "\n",
    "\n",
    "\n",
    "for index, row in questions.iterrows():\n",
    "    question = row['question']\n",
    "    process_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc87eea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>sources</th>\n",
       "      <th>top_k_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did Amazon's efforts to develop and retain...</td>\n",
       "      <td>I don't know based on the provided document. T...</td>\n",
       "      <td>2024-amazon-annual-report-10K.pdf</td>\n",
       "      <td>1; 14; 86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>During Apple Inc.’s fiscal year ended Septembe...</td>\n",
       "      <td>Based on the context you provided, the followi...</td>\n",
       "      <td>2024-apple-annual-report-10K.pdf</td>\n",
       "      <td>125; 113; 200; 201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the potential risks and challenges th...</td>\n",
       "      <td>Cisco faced several challenges in fiscal year ...</td>\n",
       "      <td>2024-cisco-full-annual-report.pdf</td>\n",
       "      <td>214; 215; 216; 218; 263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe the major regulatory and legal challe...</td>\n",
       "      <td>Alphabet Inc. faced several major regulatory a...</td>\n",
       "      <td>2024-google-annual-report-10K.pdf</td>\n",
       "      <td>347; 438; 399; 396; 345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the limitations and key consideration...</td>\n",
       "      <td>The search results do not explicitly detail th...</td>\n",
       "      <td>2024-meta-full-annual-report.pdf; 2024-reddit-...</td>\n",
       "      <td>449; 967; 442; 448; 447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the auditor's opinion on Netflix, Inc....</td>\n",
       "      <td>Based on the context you provided, the auditor...</td>\n",
       "      <td>2024-netflix-annual-report-10K.pdf</td>\n",
       "      <td>488; 495; 483; 490; 492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does NVIDIA Corporation's Board of Directo...</td>\n",
       "      <td>I don't know based on the provided document. T...</td>\n",
       "      <td>2024-nvidia-annual-report-10K.pdf</td>\n",
       "      <td>675; 585; 558; 583; 620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What types of server products does Oracle Corp...</td>\n",
       "      <td>Oracle Corporation offers two main types of se...</td>\n",
       "      <td>2024-oracle-annual-report-10K.pdf</td>\n",
       "      <td>728; 738; 726; 740; 724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What were the key consolidated financial resul...</td>\n",
       "      <td>Based on the provided context and the addition...</td>\n",
       "      <td>2024-reddit-annual-report-10K.pdf</td>\n",
       "      <td>983; 984; 982; 981; 978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Question:  \\nAccording to Tesla, Inc.’s annual...</td>\n",
       "      <td>I don't know based on the provided document. T...</td>\n",
       "      <td>2024-tsla-annual-report-10K.pdf</td>\n",
       "      <td>1091; 1067; 1016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How did Amazon's efforts to develop and retain...   \n",
       "1  During Apple Inc.’s fiscal year ended Septembe...   \n",
       "2  What are the potential risks and challenges th...   \n",
       "3  Describe the major regulatory and legal challe...   \n",
       "4  What are the limitations and key consideration...   \n",
       "5  What is the auditor's opinion on Netflix, Inc....   \n",
       "6  How does NVIDIA Corporation's Board of Directo...   \n",
       "7  What types of server products does Oracle Corp...   \n",
       "8  What were the key consolidated financial resul...   \n",
       "9  Question:  \\nAccording to Tesla, Inc.’s annual...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  I don't know based on the provided document. T...   \n",
       "1  Based on the context you provided, the followi...   \n",
       "2  Cisco faced several challenges in fiscal year ...   \n",
       "3  Alphabet Inc. faced several major regulatory a...   \n",
       "4  The search results do not explicitly detail th...   \n",
       "5  Based on the context you provided, the auditor...   \n",
       "6  I don't know based on the provided document. T...   \n",
       "7  Oracle Corporation offers two main types of se...   \n",
       "8  Based on the provided context and the addition...   \n",
       "9  I don't know based on the provided document. T...   \n",
       "\n",
       "                                             sources             top_k_chunks  \n",
       "0                  2024-amazon-annual-report-10K.pdf                1; 14; 86  \n",
       "1                   2024-apple-annual-report-10K.pdf       125; 113; 200; 201  \n",
       "2                  2024-cisco-full-annual-report.pdf  214; 215; 216; 218; 263  \n",
       "3                  2024-google-annual-report-10K.pdf  347; 438; 399; 396; 345  \n",
       "4  2024-meta-full-annual-report.pdf; 2024-reddit-...  449; 967; 442; 448; 447  \n",
       "5                 2024-netflix-annual-report-10K.pdf  488; 495; 483; 490; 492  \n",
       "6                  2024-nvidia-annual-report-10K.pdf  675; 585; 558; 583; 620  \n",
       "7                  2024-oracle-annual-report-10K.pdf  728; 738; 726; 740; 724  \n",
       "8                  2024-reddit-annual-report-10K.pdf  983; 984; 982; 981; 978  \n",
       "9                    2024-tsla-annual-report-10K.pdf         1091; 1067; 1016  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"qa_log.csv\")\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
