# Finance PDF RAG QA Evaluator

## Overview

**Finance PDF RAG QA Evaluator** is a lightweight evaluation tool designed to assess the factual accuracy and reliability of Retrieval-Augmented Generation (RAG) models working with financial reports. Given a folder of PDFs, the system auto-generates finance-specific questions using Perplexity Sonar, passes them to your RAG model for answers, and then uses Sonar again to evaluate those answers for accuracy, completeness, and clarity.

The result is a clear, actionable reportâ€”delivered in both CSV and interactive visual formatâ€”that pinpoints hallucinations and improvement areas in your RAG pipeline. Built using LangChain, PyMuPDF, and Plotly, the entire process runs locally in minutes, enabling fast iteration and trust evaluation at scale.

## Project Layout

### Top-Level Files

- **`main.ipynb`** â€“ Run-me-first notebook that demos the entire pipeline  
- **`config.py`** â€“ All pipeline settings (paths, chunk size, weights)  
- **`data_processing.py`** â€“ PDF loading and text-chunking helpers  
- **`question_generator.py`** â€“ Builds challenge questions with Perplexity Sonar  
- **`llm.py`** â€“ Light wrappers around Perplexity API (generation + judging)  
- **`evaluation.py`** â€“ Scoring logic, statistics, Plotly charts  
- **`utils.py`** â€“ Miscellaneous helpers (folder setup, timers, logging)  

### Folders

- **`data/`** â€“ Raw PDF reports  
- **`faiss_index_open/`** â€“ Sample FAISS index for a demo RAG model  
- **`output/`** â€“ Generated questions, scores, and plots  
- **`research/`** â€“ Scratch notebooks, EDA, prompt experiments  
- **`user_models/`** â€“ Toy RAG pipelines for testing  

> **Quick start:** open `main.ipynb`, point it at your PDFs, and the pipeline will create questions, grade your model, and drop results into `output/`.

> **Installation:** Run `pip install -r requirements.txt` to install all necessary packages before running the pipeline.

---

### Challenges
- **Context balance:** giving Sonar enough text so questions make sense without leaking answers.  
- **Messy data:** cleaning scanned PDFs and broken tables.  
- **Performance:** keeping evaluation fast on a laptop.  
- **Sonar gaps:** no embeddings endpoint and thin docs, so we spent extra hours researching work-arounds and prompt formats.

### Accomplishments
Despite only one teammate knowing RAG, we ramped up in 10 evenings and delivered a one-command demo, from ingest to dashboard.

### What we learned
Real documents are far messier than any tutorial; most engineering time goes to parsing and optimization, not to glamorous LLM calls.

---

## Whatâ€™s next
- **Batch evaluation:** send whole batches of question-answer pairs to the judge in one call, cutting evaluation time from minutes to seconds.  
- **PyPI package:** `pip install rag-qa-eval` for instant drop-in use.  
- **Domain plug-ins:** legal, healthcare, retail modules with their own prompts and checks.  
- **Drag-and-drop web UI:** non-devs can upload PDFs and get scores.  
- **CI badge:** build fails automatically when trust scores fall below a threshold.

---

## ðŸ“¸ Project Screenshots

- General architecture

  ![image](https://github.com/user-attachments/assets/aca39ae0-f41e-4268-9357-95c6c7658b99)
    
- Sample question-answer-evaluation results

  ![image](https://github.com/user-attachments/assets/948978d4-c51f-4ad6-b89e-9a00a36b3047)


- Evaluation results of 10 questions randomly generated by context

  ![image](https://github.com/user-attachments/assets/e3f86085-d476-4920-8b12-7130428e07f2)

- Plotly charts

  ![image](https://github.com/user-attachments/assets/ece81091-1d43-4ea2-b3f4-13da3a097385)

  ![image](https://github.com/user-attachments/assets/93544117-d413-4e53-b171-0b3300bebe33)


