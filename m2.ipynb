{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d04483",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8156c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatPerplexity\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# === Settings ===\n",
    "PDF_FOLDER = \"data\"\n",
    "CHUNK_SIZE = 5000\n",
    "CHUNK_OVERLAP = 500\n",
    "VECTOR_STORE_DIR = \"chroma_index_finance\"\n",
    "LOG_FILE = \"qa_log_2.csv\"\n",
    "\n",
    "# === Load all PDFs ===\n",
    "all_documents = []\n",
    "for filename in os.listdir(PDF_FOLDER):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(PDF_FOLDER, filename)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        for doc in documents:\n",
    "            doc.metadata[\"source\"] = filename  # track which PDF it came from\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} total documents.\")\n",
    "\n",
    "# === Split text into chunks ===\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# Add unique chunk IDs and ensure source is in metadata\n",
    "for i, doc in enumerate(chunks):\n",
    "    doc.metadata[\"chunk_id\"] = i\n",
    "    doc.metadata[\"source\"] = doc.metadata.get(\"source\", \"unknown\")\n",
    "\n",
    "# === Embedding model ===\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# === Vector Store ===\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=hf,\n",
    "    persist_directory=VECTOR_STORE_DIR\n",
    ")\n",
    "# vector_store.persist()\n",
    "\n",
    "# === Prompt Template (only answer output) ===\n",
    "prompt_template = \"\"\"\n",
    "You are a professional financial advisor with expertise in corporate finance, investment analysis, and career development in finance-related roles.\n",
    "\n",
    "Use only the information provided in the context to answer the user's question.\n",
    "Do not make assumptions or fabricate any details.\n",
    "\n",
    "Respond clearly and professionally, as if advising a client on their financial career or investment decisions.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the answer is not explicitly stated in the context, respond with: \"I don't know based on the provided document\".\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# === Retriever Setup ===\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Perplexity LLM\n",
    "perplexity_llm = ChatPerplexity(\n",
    "    model=\"sonar\",\n",
    "    pplx_api_key=\"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\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Compression retriever\n",
    "compressor = LLMChainExtractor.from_llm(perplexity_llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# === QA Chain ===\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=perplexity_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# === Process QA + Save to CSV ===\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def process_answer(query):\n",
    "    response = qa_chain({\"query\": query})\n",
    "    \n",
    "    # Handle case where result is a JSON-formatted string\n",
    "    try:\n",
    "        result = json.loads(response[\"result\"])\n",
    "        answer_text = result.get(\"answer\", response[\"result\"])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Fallback: use the raw string if not JSON\n",
    "        answer_text = response[\"result\"]\n",
    "\n",
    "    source_docs = response['source_documents']\n",
    "\n",
    "    sources_info = []\n",
    "    for doc in source_docs:\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\", \"N/A\")\n",
    "        source = doc.metadata.get(\"source\", \"N/A\")\n",
    "        sources_info.append({\"chunk_id\": str(chunk_id), \"source\": source})\n",
    "\n",
    "    # Prepare data to log\n",
    "    top_k_chunks = [src[\"chunk_id\"] for src in sources_info if src[\"chunk_id\"] != \"N/A\"]\n",
    "    sources_list = [src[\"source\"] for src in sources_info if src[\"source\"] != \"N/A\"]\n",
    "\n",
    "    # Write to CSV\n",
    "    file_exists = os.path.isfile(LOG_FILE)\n",
    "    with open(LOG_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"question\", \"answer\", \"sources\", \"top_k_chunks\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow({\n",
    "            \"question\": query,\n",
    "            \"answer\": answer_text,\n",
    "            \"sources\": \"; \".join(list(set(sources_list))),\n",
    "            \"top_k_chunks\": \"; \".join(list(set(top_k_chunks)))\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f269bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "questions= pd.read_csv(\"session_1/questions.csv\")\n",
    "\n",
    "for index, row in questions.iterrows():\n",
    "    question = row['question']\n",
    "    process_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0df12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"qa_log_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05922e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json, re\n",
    "\n",
    "API_KEY = \"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\"    \n",
    "#INITIALIZATION\n",
    "client = OpenAI(api_key=API_KEY, base_url=\"https://api.perplexity.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92153652",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a financial data Q&A evaluator.\n",
    "\n",
    "    You are given:\n",
    "    - A **question** generated from a document chunk.\n",
    "    - The **document chunk** (ground truth source).\n",
    "    - A **model-generated answer** to the question.\n",
    "\n",
    "    Your job is to score the modelâ€™s answer by carefully comparing it to the document chunk.\n",
    "\n",
    "    Use the following rubric for each category:\n",
    "\n",
    "    ---\n",
    "    **Factual Correctness**\n",
    "    - 5 = All facts are fully correct and consistent with the chunk.\n",
    "    - 4 = Minor factual inaccuracies but mostly correct.\n",
    "    - 3 = Some factual inaccuracies, partly correct.\n",
    "    - 2 = Major factual mistakes, mostly incorrect.\n",
    "    - 1 = Completely factually wrong.\n",
    "\n",
    "    ---\n",
    "    **Completeness**\n",
    "    - 5 = Fully answers the question with all key details.\n",
    "    - 4 = Mostly complete, missing minor details.\n",
    "    - 3 = Partially complete, missing important parts.\n",
    "    - 2 = Mostly incomplete, only touches on part of the question.\n",
    "    - 1 = Completely incomplete.\n",
    "\n",
    "    ---\n",
    "    3**Clarity**\n",
    "    - 5 = Clear, precise, and easy to understand.\n",
    "    - 4 = Mostly clear, with minor awkwardness.\n",
    "    - 3 = Understandable but somewhat confusing or vague.\n",
    "    - 2 = Hard to understand or poorly phrased.\n",
    "    - 1 = Completely unclear or nonsensical.\n",
    "\n",
    "    ---\n",
    "    **Response Format**\n",
    "    Return ONLY this JSON (no extra explanation):\n",
    "    {\n",
    "        \"factual_correctness_score\": [1-5],\n",
    "        \"completeness_score\": [1-5],\n",
    "        \"clarity_score\": [1-5],\n",
    "        \"comments\": \"A brief explanation (1-2 sentences) why you assigned these scores.\"\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def evaluate_answer(question, chunk, answer):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"sonar\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                Please evaluate the following answer based on the provided question and document chunk. \n",
    "                Return ONLY a valid JSON object.\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Document Chunk: {chunk}\n",
    "\n",
    "                Model Answer: {answer}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content.strip()\n",
    "    print(\"LLM Raw Output:\", response_content)\n",
    "\n",
    "    # Remove duplicate keys by keeping only the last occurrence\n",
    "    cleaned_content = re.sub(\n",
    "        r'(,\\s*\")(\\w+_score)\":\\s*\\d,\\s*\"\\2\":\\s*\\d',\n",
    "        lambda m: f',{m.group(2)}\": {m.group(0).split(\":\")[-1]}',\n",
    "        response_content\n",
    "    )\n",
    "\n",
    "    result = json.loads(cleaned_content)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare a list to collect all processed rows\n",
    "final_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    question = row['question']\n",
    "    top_k_chunk = row['top_k_chunks']\n",
    "    answer = row['answer']\n",
    "\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            evaluation = evaluate_answer(question, top_k_chunk, answer)\n",
    "            success = True  # Break loop if successful\n",
    "        except Exception as e:\n",
    "            print(f\"Retrying for question: {question} due to error: {e}\")\n",
    "\n",
    "    # Build a combined result dictionary\n",
    "    result_row = {\n",
    "        'question': question,\n",
    "        'top_k_chunk': top_k_chunk,\n",
    "        'answer': answer\n",
    "    }\n",
    "    # Add evaluation results\n",
    "    for key, value in evaluation.items():\n",
    "        result_row[f'evaluation_{key}'] = value\n",
    "\n",
    "    final_rows.append(result_row)\n",
    "\n",
    "# Convert list of results to DataFrame\n",
    "final_df = pd.DataFrame(final_rows)\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv('final.csv', index=False)\n",
    "print(\"Saved final results to final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16146022",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"final.csv\")\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
