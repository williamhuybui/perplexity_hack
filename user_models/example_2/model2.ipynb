{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a4a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatPerplexity\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# === Settings ===\n",
    "PDF_FOLDER = \"data\"\n",
    "CHUNK_SIZE = 5000\n",
    "CHUNK_OVERLAP = 500\n",
    "VECTOR_STORE_DIR = \"chroma_index_finance\"\n",
    "LOG_FILE = \"qa_log.csv\"\n",
    "\n",
    "# === Load all PDFs ===\n",
    "all_documents = []\n",
    "for filename in os.listdir(PDF_FOLDER):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(PDF_FOLDER, filename)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        for doc in documents:\n",
    "            doc.metadata[\"source\"] = filename  # track which PDF it came from\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} total documents.\")\n",
    "\n",
    "# === Split text into chunks ===\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# Add unique chunk IDs and ensure source is in metadata\n",
    "for i, doc in enumerate(chunks):\n",
    "    doc.metadata[\"chunk_id\"] = i\n",
    "    doc.metadata[\"source\"] = doc.metadata.get(\"source\", \"unknown\")\n",
    "\n",
    "# === Embedding model ===\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# === Vector Store ===\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=hf,\n",
    "    persist_directory=VECTOR_STORE_DIR\n",
    ")\n",
    "# vector_store.persist()\n",
    "\n",
    "# === Prompt Template (only answer output) ===\n",
    "prompt_template = \"\"\"\n",
    "You are a professional financial advisor with expertise in corporate finance, investment analysis, and career development in finance-related roles.\n",
    "\n",
    "Use only the information provided in the context to answer the user's question.\n",
    "Do not make assumptions or fabricate any details.\n",
    "\n",
    "Respond clearly and professionally, as if advising a client on their financial career or investment decisions.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the answer is not explicitly stated in the context, respond with: \"I don't know based on the provided document\".\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# === Retriever Setup ===\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Perplexity LLM\n",
    "perplexity_llm = ChatPerplexity(\n",
    "    model=\"sonar\",\n",
    "    pplx_api_key=\"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\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Compression retriever\n",
    "compressor = LLMChainExtractor.from_llm(perplexity_llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# === QA Chain ===\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=perplexity_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# === Process QA + Save to CSV ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e785bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def process_answer(query):\n",
    "    response = qa_chain({\"query\": query})\n",
    "    \n",
    "    # Handle case where result is a JSON-formatted string\n",
    "    try:\n",
    "        result = json.loads(response[\"result\"])\n",
    "        answer_text = result.get(\"answer\", response[\"result\"])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Fallback: use the raw string if not JSON\n",
    "        answer_text = response[\"result\"]\n",
    "\n",
    "    source_docs = response['source_documents']\n",
    "\n",
    "    sources_info = []\n",
    "    for doc in source_docs:\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\", \"N/A\")\n",
    "        source = doc.metadata.get(\"source\", \"N/A\")\n",
    "        sources_info.append({\"chunk_id\": str(chunk_id), \"source\": source})\n",
    "\n",
    "    # Prepare data to log\n",
    "    top_k_chunks = [src[\"chunk_id\"] for src in sources_info if src[\"chunk_id\"] != \"N/A\"]\n",
    "    sources_list = [src[\"source\"] for src in sources_info if src[\"source\"] != \"N/A\"]\n",
    "\n",
    "    # Write to CSV\n",
    "    file_exists = os.path.isfile(LOG_FILE)\n",
    "    with open(LOG_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"question\", \"answer\", \"sources\", \"top_k_chunks\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow({\n",
    "            \"question\": query,\n",
    "            \"answer\": answer_text,\n",
    "            \"sources\": \"; \".join(list(set(sources_list))),\n",
    "            \"top_k_chunks\": \"; \".join(list(set(top_k_chunks)))\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "questions= pd.read_csv(\"session_1/questions.csv\")\n",
    "\n",
    "for index, row in questions.iterrows():\n",
    "    question = row['question']\n",
    "    process_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"qa_log.csv\")\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
